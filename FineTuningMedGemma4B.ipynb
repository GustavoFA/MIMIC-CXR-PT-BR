{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "-ktez6mtL9SQ"
      ],
      "authorship_tag": "ABX9TyNWJl5WQTTn7AG9e+v+olEc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MedGemma4B Fine Tuning\n",
        "\n",
        "Estudo de ajuste fino do MedGemma4B visando melhorar sua geração de respostas para radiografias em PT-BR.\n",
        "\n",
        "Para esse estudo utilizei como referência o exemplo fornecido pela Google, [Notebook](https://colab.research.google.com/github/google-health/medgemma/blob/main/notebooks/fine_tune_with_hugging_face.ipynb#scrollTo=r-esHCwnQFye)."
      ],
      "metadata": {
        "id": "LVNykxwRiVx0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgbbF1jNsGoy"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade --quiet accelerate bitsandbytes transformers trl peft bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import gc\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "import shutil\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "from transformers import Trainer\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from typing import List, Tuple, Any\n",
        "\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.display import Image as IPImage, display, Markdown"
      ],
      "metadata": {
        "id": "L1h2_LEzofEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login()"
      ],
      "metadata": {
        "id": "pZndHLLs6nq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Dados\n",
        "\n",
        "Os dados devem ser separados por tipo como treino, validação e teste. E devem ser balanceados, ou seja, cada tipo deve conter uma quantidade semelhante de cada caso (ex: No finding, Edema, etc). O pré-processamento considero que já deva ser feito antes desse passo. Nesta etapa apenas iremos carregar os dados (que podem ser armazenados no Google Drive)."
      ],
      "metadata": {
        "id": "mkVPHIqlnVBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Montagem do Google Drive no ambiente do Colab"
      ],
      "metadata": {
        "id": "i9u9OJgGpJYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7NskCIlTo-RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copiar arquivos para o ambiente do Colab"
      ],
      "metadata": {
        "id": "5TRSIqL0Dsx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # caminho para repositório mini - apenas para testar funcionamento do notebook\n",
        "# files_path = '/content/drive/MyDrive/2025_2S_IA368HH/Projeto Final/SubDataset - MIMIC-CXR/micro_dataset_mimic_balanced/version1'"
      ],
      "metadata": {
        "id": "3n48YFeFCSWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_path = '/content/drive/MyDrive/2025_2S_IA368HH/Projeto Final/SubDataset - MIMIC-CXR/last_micro_dataset_mimic_balanced'"
      ],
      "metadata": {
        "id": "1_FVxmqEOq8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# !cp -r '' /content/\n",
        "if not os.path.exists(\"/content/Dataset\"):\n",
        "    shutil.copytree(files_path, \"/content/Dataset\")"
      ],
      "metadata": {
        "id": "Lghn-4zpCXWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminhos para cada conjunto de itens dos arquivos importados\n",
        "train_img_dir = r'/content/Dataset/train/images'\n",
        "train_txt_dir = r'/content/Dataset/train/texts'\n",
        "val_img_dir = r'/content/Dataset/validate/images'\n",
        "val_txt_dir = r'/content/Dataset/validate/texts'"
      ],
      "metadata": {
        "id": "gaI17GT-Emzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funções para carregar imagens e textos"
      ],
      "metadata": {
        "id": "14gYAGgUEF7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder:str) -> list[list]:\n",
        "  ''' Load images from Colab environment files '''\n",
        "  all_images = []\n",
        "\n",
        "  for subfolder in sorted(os.listdir(folder)):\n",
        "      subfolder_path = os.path.join(folder, subfolder)\n",
        "\n",
        "      if not os.path.isdir(subfolder_path):\n",
        "          continue\n",
        "\n",
        "      sub_images = []\n",
        "      for filename in sorted(os.listdir(subfolder_path)):\n",
        "          path = os.path.join(subfolder_path, filename)\n",
        "          try:\n",
        "              img = Image.open(path)\n",
        "              sub_images.append(img)\n",
        "          except Exception as e:\n",
        "              print(f\"Error to load {path}: {e}\")\n",
        "      if sub_images:\n",
        "          all_images.append(sub_images)\n",
        "\n",
        "  return all_images"
      ],
      "metadata": {
        "id": "5HBGlPbKpPPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_texts_from_folder(folder:str) -> list:\n",
        "  ''' Load texts from Colab environment '''\n",
        "  texts = []\n",
        "  for filename in sorted(os.listdir(folder)):\n",
        "      path = os.path.join(folder, filename)\n",
        "      if filename.endswith(\".txt\"):\n",
        "          with open(path, 'r', encoding='utf-8') as f:\n",
        "              texts.append(f.read().strip())\n",
        "  return texts"
      ],
      "metadata": {
        "id": "SUMg7dz8ELcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separando os dados de cada tipo"
      ],
      "metadata": {
        "id": "TCJ1uHV5_FW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = load_images_from_folder(train_img_dir)\n",
        "train_texts  = load_texts_from_folder(train_txt_dir)\n",
        "val_images   = load_images_from_folder(val_img_dir)\n",
        "val_texts    = load_texts_from_folder(val_txt_dir)"
      ],
      "metadata": {
        "id": "BPRAYRGNEOl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train: {len(train_images)} imagens, {len(train_texts)} textos\")\n",
        "print(f\"Val: {len(val_images)} imagens, {len(val_texts)} textos\")"
      ],
      "metadata": {
        "id": "iLTc8nouES7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificando uma amostra dos dados"
      ],
      "metadata": {
        "id": "dPexu-ce0TwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = random.randint(0, len(train_images)-1)"
      ],
      "metadata": {
        "id": "WlbCoxrI1J46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_images[n][0])"
      ],
      "metadata": {
        "id": "yg1ouD3C0tNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(train_texts[n])"
      ],
      "metadata": {
        "id": "I1s1aYoL1AD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Processador dos dados"
      ],
      "metadata": {
        "id": "FLCcYY_K0eHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo do Processador"
      ],
      "metadata": {
        "id": "Bly_4EEl_o_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'google/medgemma-4b-it'"
      ],
      "metadata": {
        "id": "2JZ3_E80YzTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicialização"
      ],
      "metadata": {
        "id": "fB1e6Q1H_ubw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicialização do processador de dados do MedGemma4B\n",
        "# (esse é o modelo usado no Colab de exemplo da Google)\n",
        "processor_4b = AutoProcessor.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "Hd-o2FHG6G1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sugestão no notebook da Google - usar padding no final das sequências de texto."
      ],
      "metadata": {
        "id": "WNJYnMRN_wqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use right padding to avoid issues during training\n",
        "processor_4b.tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "vc1a_lFGY8OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Dataset\n",
        "\n",
        "Para o dataset teremos a separação das imagens e dos textos esperados na saída (target). A formatação da mensagem será feito apenas no getitem."
      ],
      "metadata": {
        "id": "Oo7i6iwCobdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageAndStudyDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch dataset that groups multiple radiology images from the same study\n",
        "    together with their corresponding medical report, formatting the output in\n",
        "    the multimodal chat structure required by MedGemma4B.\n",
        "\n",
        "    Each dataset entry represents a single radiology study and provides:\n",
        "      - images: a list of PIL.Image objects belonging to the study\n",
        "      - label: the ground-truth medical report associated with that study\n",
        "      - messages: a chat-style multimodal prompt containing:\n",
        "            * one {\"type\": \"image\"} entry for each image\n",
        "            * a system/user text prompt\n",
        "            * the medical report as the assistant response\n",
        "\n",
        "    The dataset expects the following directory structure:\n",
        "        files_path/\n",
        "            images/\n",
        "                study_001/\n",
        "                    img1.png\n",
        "                    img2.png\n",
        "                    ...\n",
        "                study_002/\n",
        "                    ...\n",
        "            texts/\n",
        "                report_001.txt\n",
        "                report_002.txt\n",
        "                ...\n",
        "\n",
        "    Input:\n",
        "      - files_path (str): base directory containing 'images/' and 'texts/'.\n",
        "      - sys_prompt (str): text instruction included in the user message.\n",
        "\n",
        "    Output (per item):\n",
        "      A dictionary with:\n",
        "        {\n",
        "            'image': [PIL.Image, ...],\n",
        "            'label': <string medical report>,\n",
        "            'messages': <multimodal chat prompt>\n",
        "        }\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        images,\n",
        "        texts,\n",
        "        sys_prompt: str = \"Apresente o diagnóstico das imagens de radiografia, em português brasileiro\",\n",
        "        filter_text:bool=True,\n",
        "        lim_images:bool=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        self.images = images\n",
        "        self.reports = texts\n",
        "        self.sys_prompt = sys_prompt\n",
        "        self.filter_text = filter_text\n",
        "        self.lim_images = lim_images\n",
        "\n",
        "    # def load_texts_from_folder(self, folder:str) -> list:\n",
        "    #     ''' Load texts from Colab environment '''\n",
        "    #     texts = []\n",
        "    #     for filename in sorted(os.listdir(folder)):\n",
        "    #         path = os.path.join(folder, filename)\n",
        "    #         if filename.endswith(\".txt\"):\n",
        "    #             with open(path, 'r', encoding='utf-8') as f:\n",
        "    #                 texts.append(f.read().strip())\n",
        "    #     return texts\n",
        "\n",
        "    # def load_images_from_folder(self, folder:str) -> list[list]:\n",
        "    #     ''' Load images from Colab environment files '''\n",
        "    #     all_images = []\n",
        "\n",
        "    #     for subfolder in sorted(os.listdir(folder)):\n",
        "    #         subfolder_path = os.path.join(folder, subfolder)\n",
        "\n",
        "    #         if not os.path.isdir(subfolder_path):\n",
        "    #             continue\n",
        "\n",
        "    #         sub_images = []\n",
        "    #         for filename in sorted(os.listdir(subfolder_path)):\n",
        "    #             path = os.path.join(subfolder_path, filename)\n",
        "    #             try:\n",
        "    #                 img = Image.open(path)\n",
        "    #                 sub_images.append(img)\n",
        "    #             except Exception as e:\n",
        "    #                 print(f\"Error to load {path}: {e}\")\n",
        "    #         if sub_images:\n",
        "    #             all_images.append(sub_images)\n",
        "\n",
        "    #     return all_images\n",
        "\n",
        "    def text_filter(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Extracts the ACHADOS + IMPRESSÃO sections from a medical report\n",
        "        and removes markdown markers (**, ---) and section headers.\n",
        "        \"\"\"\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Locate \"ACHADOS\"\n",
        "        match_findings = re.search(r'\\bachados\\b\\s*:', text_lower)\n",
        "        if not match_findings:\n",
        "            return \"\"\n",
        "\n",
        "        start = match_findings.end()\n",
        "\n",
        "        # Locate \"OBSERVAÇÕES\" to use as stopping point\n",
        "        match_observations = re.search(r'\\bobserva[cç][oõ]es\\b\\s*:', text_lower)\n",
        "        end = match_observations.start() if match_observations else len(text)\n",
        "\n",
        "        # Extract the raw content between ACHADOS and OBSERVAÇÕES\n",
        "        extracted = text[start:end].strip()\n",
        "\n",
        "        # Patterns to clean markdown symbols and section titles\n",
        "        cleanup_patterns = [\n",
        "            r\"\\*\\*\",                    # remove ** markdown\n",
        "            r\"^achados\\s*:\\s*\",         # remove 'ACHADOS:'\n",
        "            r\"^impress[aã]o\\s*:\\s*\",    # remove 'IMPRESSÃO:'\n",
        "            r\"---\",                     # remove horizontal rule\n",
        "        ]\n",
        "\n",
        "        for pattern in cleanup_patterns:\n",
        "            extracted = re.sub(pattern, \"\", extracted, flags=re.IGNORECASE | re.MULTILINE)\n",
        "\n",
        "        # Normalize extra blank lines\n",
        "        extracted = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", extracted).strip()\n",
        "\n",
        "        return extracted\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a single dataset item formatted according to the\n",
        "        multimodal format expected by the MedGemma processor.\n",
        "\n",
        "        The \"messages\" structure follows this logic:\n",
        "            - Each image is represented as {\"type\": \"image\"}.\n",
        "            - After all images, the system/user prompt is added as text.\n",
        "            - The medical report is provided as the assistant's response.\n",
        "        \"\"\"\n",
        "\n",
        "        report = self.reports[idx]\n",
        "        if self.filter_text:\n",
        "            report = self.text_filter(report)\n",
        "\n",
        "        # Create one {\"type\": \"image\"} entry for each image in the study\n",
        "        images = self.images[idx]\n",
        "        type_images = [{'type': 'image'} for _ in range(len(images))]\n",
        "        # limit the number of images (max: 2 images)\n",
        "        if self.lim_images:\n",
        "            images = images[:2]\n",
        "            type_images = type_images[:2]\n",
        "\n",
        "        return {\n",
        "            'image': images,   # list of PIL images for this study\n",
        "            'label': report,   # ground-truth medical report\n",
        "            'messages': [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": type_images + [\n",
        "                        {'type': 'text', 'text': self.sys_prompt}\n",
        "                    ],\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": [\n",
        "                        {'type': 'text', 'text': report},\n",
        "                    ],\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of radiology studies in the dataset.\"\"\"\n",
        "        return len(self.images)"
      ],
      "metadata": {
        "id": "OJQJ9KQ5O8dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class ImageAndStudyDataset(Dataset):\n",
        "#     \"\"\"\n",
        "#     Dataset for training MedGemma4B,combining multiple images from a single\n",
        "#     radiology study with the corresponding medical report.\n",
        "\n",
        "#     Each dataset item contains:\n",
        "#       - images: list of PIL images belonging to the same study\n",
        "#       - label: the ground-truth radiology report\n",
        "#       - messages: a chat-style structure (role/content) used by the processor\n",
        "#                   to generate a multimodal prompt with N images + text.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         images: list[list],\n",
        "#         reports: list[str],\n",
        "#         sys_prompt: str = \"Apresente o diagnóstico das imagens de radiografia, em português brasileiro\"\n",
        "#     ):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             images (list[list[PIL.Image]]):\n",
        "#                 A list where each element corresponds to one radiology study.\n",
        "#                 Each study contains a list of one or more PIL images.\n",
        "\n",
        "#             reports (list[str]):\n",
        "#                 A list of textual medical reports that match the image groups.\n",
        "\n",
        "#             sys_prompt (str):\n",
        "#                 Instruction text appended after all image tokens in the prompt.\n",
        "#                 This is shown as the user input before the model generates a response.\n",
        "#         \"\"\"\n",
        "#         self.images = images\n",
        "#         self.reports = reports\n",
        "#         self.sys_prompt = sys_prompt\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         \"\"\"\n",
        "#         Returns a single dataset item formatted according to the\n",
        "#         multimodal format expected by the MedGemma processor.\n",
        "\n",
        "#         The \"messages\" structure follows this logic:\n",
        "#             - Each image is represented as {\"type\": \"image\"}.\n",
        "#             - After all images, the system/user prompt is added as text.\n",
        "#             - The medical report is provided as the assistant's response.\n",
        "#         \"\"\"\n",
        "#         images = self.images[idx]\n",
        "#         report = self.reports[idx]\n",
        "\n",
        "#         # Create one {\"type\": \"image\"} entry for each image in the study\n",
        "#         type_images = [{'type': 'image'} for _ in range(len(images))]\n",
        "\n",
        "#         return {\n",
        "#             'image': images,   # list of PIL images for this study\n",
        "#             'label': report,   # ground-truth medical report\n",
        "#             'messages': [\n",
        "#                 {\n",
        "#                     \"role\": \"user\",\n",
        "#                     \"content\": type_images + [\n",
        "#                         {'type': 'text', 'text': self.sys_prompt}\n",
        "#                     ],\n",
        "#                 },\n",
        "#                 {\n",
        "#                     \"role\": \"assistant\",\n",
        "#                     \"content\": [\n",
        "#                         {'type': 'text', 'text': report},\n",
        "#                     ],\n",
        "#                 }\n",
        "#             ]\n",
        "#         }\n",
        "\n",
        "#     def __len__(self):\n",
        "#         \"\"\"Returns the number of radiology studies in the dataset.\"\"\"\n",
        "#         return len(self.images)\n"
      ],
      "metadata": {
        "id": "0_Pcr8OnJbby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageAndStudyDataset(train_images, train_texts)\n",
        "val_dataset = ImageAndStudyDataset(val_images, val_texts)"
      ],
      "metadata": {
        "id": "11pk-12qAbbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificando uma amostra do dataset"
      ],
      "metadata": {
        "id": "tzSg_U4xE-Ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = train_dataset[random.randint(0, len(train_dataset)-1)]"
      ],
      "metadata": {
        "id": "o2lXixUICktg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_dataset)"
      ],
      "metadata": {
        "id": "q92YRx-X6G6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = output['label']\n",
        "images = output['image']\n",
        "message = output['messages']"
      ],
      "metadata": {
        "id": "TCgLmR9nnDJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(images), type(text), type(message)"
      ],
      "metadata": {
        "id": "LXLhzqDL1Ydb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "5VyGJAe0jMX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "AXvR9vsOnMpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(*images)"
      ],
      "metadata": {
        "id": "r316FieRCsRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message"
      ],
      "metadata": {
        "id": "rDnHjdOyC4-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Amostrador\n"
      ],
      "metadata": {
        "id": "YGuUY0TXo-z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def medgemma_collate_fn(examples: list[dict[str, Any]], processor=processor_4b):\n",
        "    \"\"\"\n",
        "    Custom collate function for preparing multimodal batches for Med-Gemma 4B.\n",
        "\n",
        "    This function:\n",
        "    - Extracts text messages and images from each example.\n",
        "    - Applies the chat template to convert message dictionaries into textual prompts.\n",
        "    - Uses the MedGemma processor to tokenize text and preprocess images.\n",
        "    - Creates label tensors for supervised training.\n",
        "    - Masks out special tokens such as padding, image placeholder tokens,\n",
        "      and specific unused tokens so they do not contribute to the loss.\n",
        "\n",
        "    Args:\n",
        "        examples (list[dict]): A list of dataset samples. Each sample is expected to contain:\n",
        "            - \"image\": a list of images corresponding to the example.\n",
        "            - \"messages\": a list of dictionaries representing a chat-like conversation.\n",
        "        processor: MedGemma processor responsible for tokenization and image preprocessing.\n",
        "\n",
        "    Returns:\n",
        "        dict: A batch dictionary containing:\n",
        "            - \"input_ids\": tokenized text\n",
        "            - \"pixel_values\": processed image tensors\n",
        "            - \"attention_mask\": attention mask for text\n",
        "            - \"labels\": training labels with ignored positions masked to -100\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    images = []\n",
        "\n",
        "    # Iterate over the batch and extract text prompts and images\n",
        "    for example in examples:\n",
        "        # Append the image list to the images array (each example contains a list of images)\n",
        "        images.append(example['image'])\n",
        "\n",
        "        # Convert the chat-style message into a plain text prompt (no tokenization yet)\n",
        "        texts.append(processor.apply_chat_template(\n",
        "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
        "        ).strip())\n",
        "\n",
        "    # Use the processor to tokenize the texts and preprocess the images\n",
        "    # padding=True ensures that text sequences are padded to the same length\n",
        "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Clone input IDs to create labels for supervised training\n",
        "    labels = batch[\"input_ids\"].clone()\n",
        "\n",
        "    # Get token ID corresponding to the <image> placeholder token\n",
        "    image_token_id = [\n",
        "        processor.tokenizer.convert_tokens_to_ids(\n",
        "            processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Ignore padded tokens in the loss\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    # Ignore special image placeholder tokens in the loss\n",
        "    labels[labels == image_token_id] = -100\n",
        "\n",
        "    # Ignore an additional special token (specific to the MedGemma4B tokenizer)\n",
        "    labels[labels == 262144] = -100\n",
        "\n",
        "    # Add labels to the batch\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch\n"
      ],
      "metadata": {
        "id": "EgkeQ5DmKBoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificando a função medgemma_collate_fn"
      ],
      "metadata": {
        "id": "CzltudxkIEv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_batch = [train_dataset[i] for i in range(2)]"
      ],
      "metadata": {
        "id": "VpszLO25IG1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_batch[0]"
      ],
      "metadata": {
        "id": "xcZ-CVTeIrw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(*sample_batch[0]['image'])"
      ],
      "metadata": {
        "id": "N__TDa4LwrlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_output = medgemma_collate_fn(sample_batch)"
      ],
      "metadata": {
        "id": "c-bPFsOAIL73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in batch_output.items():\n",
        "    if torch.is_tensor(v):\n",
        "        print(f\"{k}: {v.dtype}\")"
      ],
      "metadata": {
        "id": "vESLm6RkLEq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANTE : Temos que passar as imagens para torch.bfloat16"
      ],
      "metadata": {
        "id": "L1WyhJmMxUJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(batch_output.keys())\n",
        "print(batch_output[\"input_ids\"].shape)\n",
        "print(batch_output[\"attention_mask\"].shape)\n",
        "print(batch_output[\"token_type_ids\"].shape)\n",
        "print(batch_output[\"pixel_values\"].shape)\n",
        "print(batch_output[\"labels\"].shape)"
      ],
      "metadata": {
        "id": "0xCejuipKkg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Verificação do Hardware"
      ],
      "metadata": {
        "id": "RCZRVMjUP_DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "uODbkCIE3TM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ponto importante para verificar sobre capacidade do Hardware (caso não possua VRAM suficiente o modelo não consegue ser carregado)"
      ],
      "metadata": {
        "id": "3bZO_V4TF7D8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU supports bfloat16\n",
        "if torch.cuda.get_device_capability()[0] < 8:\n",
        "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")"
      ],
      "metadata": {
        "id": "KpMeylR4QBjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## QLoRA - processo manual\n",
        "\n",
        "Nesta etapa será configurado os parâmetros para o LoRA e quantização. A parte de treinamento será feito por uma função construída abaixo."
      ],
      "metadata": {
        "id": "-ktez6mtL9SQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparação do Modelo com QLoRA"
      ],
      "metadata": {
        "id": "zayM2i4RP2P_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "s1l4NBqWQNgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quantização"
      ],
      "metadata": {
        "id": "JYf1Dqv3QvAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\", # normal float 4 - default QLoRA\n",
        "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
        "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
        ")"
      ],
      "metadata": {
        "id": "XsTApmxAQNgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inicialização do Modelo"
      ],
      "metadata": {
        "id": "uRRfggY3QysQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_4b = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)"
      ],
      "metadata": {
        "id": "PP1bptcmQZd9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parâmetros para LoRA"
      ],
      "metadata": {
        "id": "jPdtZYtjQ0-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seleção das camadas para aplicação do LoRA"
      ],
      "metadata": {
        "id": "6-NwWsJjG7Mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LORA_TARGET = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]"
      ],
      "metadata": {
        "id": "D9zKJU3v0s7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,                         # rank do adaptador\n",
        "    lora_alpha=32,                # escala\n",
        "    target_modules=LORA_TARGET,\n",
        "    lora_dropout=0.05,            # dropout\n",
        "    bias=\"none\",                  # Sem bias\n",
        "    task_type=\"CAUSAL_LM\"         # causal LM (mesmo multimodal)\n",
        ")"
      ],
      "metadata": {
        "id": "weCbhmOeQ3ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Aplicação do LoRA e congelamento de algumas camadas"
      ],
      "metadata": {
        "id": "hS1t6HHZQ7n6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Caso queira resetar o modelo execute os comandos abaixo antes\n",
        "# del model\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()\n",
        "# print(f\"Memória usada: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "# print(f\"Memória reservada: {torch.cuda.memory_reserved()/1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "cPPfrSjhzjYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aplicando LoRA\n",
        "model = get_peft_model(model_4b, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "8XF46LY3Q9bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congelando parte Visual do modelo - como pretendo melhorar sua geração de texto, logo, não precisamos atualizar as camadas relacionadas à imagens."
      ],
      "metadata": {
        "id": "GBrb3T8z9NPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.vision_tower.named_parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "7tDe5yIX9Ocs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEMA : Ao tentar fazer o backpropagation ocorria um erro relacionado à gradient checkpoint. Como recomendação do ChatGPT temos que desabilitar o gradient checkpointing e uso de cache."
      ],
      "metadata": {
        "id": "WExdwxqLKGjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_disable()"
      ],
      "metadata": {
        "id": "pnQAOK0Z8XL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "5Pi0kVQ38d-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantidade de parâmetros treináveis"
      ],
      "metadata": {
        "id": "_PVUeb6OMUn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parâmetros treináveis\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "MaajCdh5Q_OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ],
      "metadata": {
        "id": "3_Lh0lv0RNsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 2e-4"
      ],
      "metadata": {
        "id": "kt3BkvTD5Jty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH = 1 # poucas épocas apenas para testar se executa sem acusar erros"
      ],
      "metadata": {
        "id": "5dE_LXGz4-ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Verificando funcionamento - se os dados inseridos no modelo são aceitos e se consegue calcular a Loss"
      ],
      "metadata": {
        "id": "mIrhnkIw1YYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  batch_output = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch_output.items()}\n",
        "  outputs = model(**batch_output)"
      ],
      "metadata": {
        "id": "zPIVJInG1dKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Loss: {outputs.loss.item():.2f}')"
      ],
      "metadata": {
        "id": "aA6KJE5G1nqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinamento"
      ],
      "metadata": {
        "id": "XyCbQpS_4UIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durante o treinamento temos que a VRAM chega próxima à 36GB."
      ],
      "metadata": {
        "id": "z7__OlMS2dVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Otimizador\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "NJoOjmMwRPvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tuning_process(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    learning_rate=5e-5,\n",
        "    epochs=3,\n",
        "    model_name_saved=\"medgemma4b_adjusted\"\n",
        "):\n",
        "  \"\"\"\n",
        "    Fine Tuning do MedGemma4B\n",
        "\n",
        "    Args:\n",
        "        model: modelo PyTorch (ex: AutoModelForImageTextToText)\n",
        "        train_loader: DataLoader com os dados de treino\n",
        "        val_loader: DataLoader com os dados de validação\n",
        "        learning_rate (float): taxa de aprendizado utilizada no AdamW\n",
        "        epochs (int): número de épocas\n",
        "        model_name_saved (str): caminho/identificador para salvar o melhor modelo\n",
        "\n",
        "    Returns:\n",
        "        train_losses (list): perdas médias de treino por época\n",
        "        val_losses (list): perdas médias de validação por época\n",
        "  \"\"\"\n",
        "\n",
        "  device = next(model.parameters()).device\n",
        "  train_losses, val_losses = [], []\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  best_val_loss = float(\"inf\")\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      # ======== TREINAMENTO ========\n",
        "      model.train()\n",
        "      total_train_loss = 0\n",
        "      pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "      for batch in pbar:\n",
        "          batch = {\n",
        "              k: (\n",
        "                  v.to(device, dtype=torch.bfloat16) if k == \"pixel_values\"\n",
        "                  else v.to(device, dtype=torch.long)\n",
        "              ) if torch.is_tensor(v) else v\n",
        "              for k, v in batch.items()\n",
        "          }\n",
        "\n",
        "          outputs = model(**batch)\n",
        "          loss = outputs.loss\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          total_train_loss += loss.item()\n",
        "          pbar.set_postfix({\"train_loss\": loss.item()})\n",
        "\n",
        "      avg_train_loss = total_train_loss / len(train_loader)\n",
        "      train_losses.append(avg_train_loss)\n",
        "\n",
        "      # ======== VALIDAÇÃO ========\n",
        "      model.eval()\n",
        "      total_val_loss = 0\n",
        "      with torch.no_grad():\n",
        "          for batch in val_loader:\n",
        "              batch = {\n",
        "                  k: (\n",
        "                      v.to(device, dtype=torch.bfloat16) if k == \"pixel_values\"\n",
        "                      else v.to(device, dtype=torch.long)\n",
        "                  ) if torch.is_tensor(v) else v\n",
        "                  for k, v in batch.items()\n",
        "              }\n",
        "\n",
        "              outputs = model(**batch)\n",
        "              total_val_loss += outputs.loss.item()\n",
        "\n",
        "      avg_val_loss = total_val_loss / len(val_loader)\n",
        "      val_losses.append(avg_val_loss)\n",
        "\n",
        "      print(f\"\\nEpoch {epoch+1}: train_loss={avg_train_loss:.4f}, val_loss={avg_val_loss:.4f}\")\n",
        "\n",
        "      # ======== SALVAR MELHOR MODELO ========\n",
        "      if avg_val_loss < best_val_loss:\n",
        "          best_val_loss = avg_val_loss\n",
        "          model.save_pretrained(model_name_saved) # salvo modelo completo (com QLoRA)\n",
        "          print(f\"Novo melhor modelo salvo em '{model_name_saved}' (val_loss={best_val_loss:.4f})\")\n",
        "      else:\n",
        "          print(\"Nenhuma melhoria na validação nesta época.\")\n",
        "\n",
        "  print(f\"\\nTreinamento finalizado! Melhor val_loss = {best_val_loss:.4f}\")\n",
        "  return train_losses, val_losses"
      ],
      "metadata": {
        "id": "zhPuKDvr1ry9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses = fine_tuning_process(model)"
      ],
      "metadata": {
        "id": "3xvY60J-7OxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot dos resultados"
      ],
      "metadata": {
        "id": "gg7RcijQN3-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(train_losses:list=train_losses, val_losses:list=val_losses):\n",
        "  epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "  axes[0].plot(epochs, train_losses, marker='o', label='Train Loss', color='royalblue')\n",
        "  axes[0].set_title(\"Training Loss\")\n",
        "  axes[0].set_xlabel(\"Epoch\")\n",
        "  axes[0].set_ylabel(\"Loss\")\n",
        "  axes[0].grid(True, linestyle='--', alpha=0.5)\n",
        "  axes[0].legend()\n",
        "\n",
        "  axes[1].plot(epochs, val_losses, marker='s', label='Validation Loss', color='tomato')\n",
        "  axes[1].set_title(\"Validation Loss\")\n",
        "  axes[1].set_xlabel(\"Epoch\")\n",
        "  axes[1].set_ylabel(\"Loss\")\n",
        "  axes[1].grid(True, linestyle='--', alpha=0.5)\n",
        "  axes[1].legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "h6lgG_1EN_6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Para carregar o modelo e utilizá-lo"
      ],
      "metadata": {
        "id": "VWyQL6XlPeP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processor = AutoProcessor.from_pretrained(\"medgemma4b_adjusted\")\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    \"medgemma4b_adjusted\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "pKuRCRi1QKE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## QLoRA - Google Example\n",
        "\n",
        "Tentativa de QLoRA utilizando o notebook exemplo da Google."
      ],
      "metadata": {
        "id": "z1gwGbnT2mr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Configuração do modelo multimodal"
      ],
      "metadata": {
        "id": "344wQaiznrgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "RHBY4KFX2qpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
        "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
        ")"
      ],
      "metadata": {
        "id": "njqUGPbiYuot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)"
      ],
      "metadata": {
        "id": "PUiJu-zYYwhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use right padding to avoid issues during training\n",
        "processor_4b.tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "I7tId5FAFuvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Configuração do LoRA"
      ],
      "metadata": {
        "id": "ULl13zwNZJ67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    target_modules=\"all-linear\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=[\n",
        "        \"lm_head\",\n",
        "        \"embed_tokens\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "e6x_dLH3ZLNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_epochs = 1  # @param {type: \"number\"}\n",
        "learning_rate = 2e-4  # @param {type: \"number\"}"
      ],
      "metadata": {
        "id": "jaj92JETZP9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Argumentos"
      ],
      "metadata": {
        "id": "94GVBe_eVXPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = r'/content/drive/MyDrive/2025_2S_IA368HH/Projeto Final/Colabs/medgemma-4b-it-lora1'"
      ],
      "metadata": {
        "id": "ttvAuD8IqlMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = SFTConfig(\n",
        "    output_dir=save_path,                                    # Directory and Hub repository id to save the model to\n",
        "    num_train_epochs=num_train_epochs,                       # Number of training epochs\n",
        "    per_device_train_batch_size=1,                           # Batch size per device during training\n",
        "    per_device_eval_batch_size=1,                            # Batch size per device during evaluation\n",
        "    gradient_accumulation_steps=16,                          # Number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True,                             # Enable gradient checkpointing to reduce memory usage\n",
        "    optim=\"adamw_torch_fused\",                               # Use fused AdamW optimizer for better performance\n",
        "    logging_steps=1,                                         # Number of steps between logs\n",
        "    save_strategy=\"epoch\",                                   # Save checkpoint every epoch\n",
        "    eval_strategy=\"steps\",                                   # Evaluate every `eval_steps`\n",
        "    eval_steps=1,                                            # Number of steps between evaluations\n",
        "    learning_rate=learning_rate,                             # Learning rate based on QLoRA paper\n",
        "    bf16=True,                                               # Use bfloat16 precision\n",
        "    max_grad_norm=0.3,                                       # Max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                                       # Warmup ratio based on QLoRA paper\n",
        "    lr_scheduler_type=\"linear\",                              # Use linear learning rate scheduler\n",
        "    push_to_hub=False,                                       # Push model to Hub (NÃO PRECISA SER ENVIADO PARA O HUGGING FACE)\n",
        "    report_to=\"tensorboard\",                                 # Report metrics to tensorboard\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Set gradient checkpointing to non-reentrant to avoid issues\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},           # Skip default dataset preparation to preprocess manually\n",
        "    remove_unused_columns = False,                           # Columns are unused for training but needed for data collator\n",
        "    label_names=[\"labels\"],                                  # Input keys that correspond to the labels\n",
        ")"
      ],
      "metadata": {
        "id": "0ytdq15bZbwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    peft_config=peft_config,\n",
        "    data_collator=medgemma_collate_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "XJFxyBosZevY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "gAVZCUhsZfuw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}