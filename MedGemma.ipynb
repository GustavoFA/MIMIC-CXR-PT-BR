{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "u5BpJ2GqXFQl",
        "VEDSjqvdMc8n",
        "uOZ5F1c7PtzV",
        "CF-ffWPY5znZ"
      ],
      "authorship_tag": "ABX9TyOxe4LD7fcsBjL0huchL+Hh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet accelerate bitsandbytes transformers"
      ],
      "metadata": {
        "id": "_SIrpispDNkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login\n",
        "from IPython.display import Image as IPImage, display, Markdown # parte mais visual\n",
        "from transformers import AutoTokenizer, AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "9mN10VNSDQMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## MedGemma Class\n",
        "\n",
        "Classe para configuração e uso dos modelos da Google, MedGemma."
      ],
      "metadata": {
        "id": "q007EQDvDYU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version1"
      ],
      "metadata": {
        "id": "u5BpJ2GqXFQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UsingMedGemma():\n",
        "\n",
        "  models = {\n",
        "      'pre-trained' : 'google/medgemma-4b-pt',\n",
        "      'instruct-tuned' : 'google/medgemma-4b-it',\n",
        "      '27b-multimodal' : 'google/medgemma-27b-it', # versão Multimodal\n",
        "      '27b-text-only' : 'google/medgemma-27b-text-it' # versão Texto\n",
        "  }\n",
        "\n",
        "  def __init__(self, model_version='instruct-tuned', device:str=None, quantization:bool=False):\n",
        "\n",
        "    # verificação do device\n",
        "    self.device = self.select_device(device)\n",
        "    # obtém versão do MedGemma\n",
        "    self.model_version = self.models[model_version]\n",
        "    # selecionando parâmetros do modelo\n",
        "    self.model_params = self.config_model_params(self.device, quantization)\n",
        "\n",
        "    # seleção da classe para o modelo\n",
        "    # versão 4B\n",
        "    if '4b' in self.model_version.lower():\n",
        "      # inicializando o modelo multimodal\n",
        "      self.model = AutoModelForImageTextToText.from_pretrained(\n",
        "          self.model_version,\n",
        "          **self.model_params\n",
        "      )\n",
        "      # inicializando o processor\n",
        "      self.processor = AutoProcessor.from_pretrained(\n",
        "        self.model_version,\n",
        "        use_fast=False\n",
        "      )\n",
        "    # versão 27B-text-only\n",
        "    else:\n",
        "      self.model = AutoModelForCausalLM.from_pretrained(\n",
        "          self.model_version,\n",
        "          **self.model_params # devemos modificar os parâmetros\n",
        "      )\n",
        "      self.processor = AutoTokenizer.from_pretrained( # Tokenizer (only-text)\n",
        "          self.model_version\n",
        "      )\n",
        "\n",
        "\n",
        "  def send_only_text(self, text:str, sys_prompt:str=None, max_tokens:int=300, show_result:bool=True) -> str:\n",
        "    ''' Geração de texto com MedGemma tendo como entrada apenas texto'''\n",
        "    # estrutura de mensagem\n",
        "    message = [\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': [\n",
        "                {\n",
        "                    'type': 'text',\n",
        "                    'text': text\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    # inserir prompt de sistema (opcional)\n",
        "    if sys_prompt is not None:\n",
        "      message.insert(0, {\n",
        "          'role': 'system',\n",
        "          'content': [\n",
        "              {\n",
        "                  'type': 'text',\n",
        "                  'text': sys_prompt\n",
        "              }\n",
        "          ]\n",
        "      })\n",
        "\n",
        "    # Preparação da entrada para o modelo\n",
        "    # versão para 4B\n",
        "    if \"4b\" in self.model_version.lower():\n",
        "      inputs = self.processor.apply_chat_template(\n",
        "          message,\n",
        "          add_generation_prompt=True,\n",
        "          tokenize=True,\n",
        "          return_dict=True,\n",
        "          return_tensors=\"pt\",\n",
        "      ).to(self.model.device)\n",
        "      # anotação : bfloat16 apenas para modelos multimodais\n",
        "    # versão para 27B\n",
        "    else:\n",
        "      inputs = self.processor.apply_chat_template(\n",
        "          message,\n",
        "          add_generation_prompt=True,\n",
        "          tokenize=True,\n",
        "          return_dict=True,\n",
        "          return_tensors=\"pt\",\n",
        "      ).to(self.model.device)\n",
        "\n",
        "    # tamanho da entrada (para removermos dos tokens de saída)\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "    # geração\n",
        "    with torch.inference_mode():\n",
        "      generation = self.model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False)\n",
        "      generation = generation[0][input_len:]\n",
        "\n",
        "    # decodificação\n",
        "    answer = self.processor.decode(generation, skip_special_tokens=True)\n",
        "    # visualização\n",
        "    if show_result:\n",
        "      self.show_results(text, answer)\n",
        "\n",
        "    return answer\n",
        "\n",
        "  def send_image_and_text(self, text, image_inputs, sys_prompt=None, max_tokens=256, show_result=True):\n",
        "    \"\"\"Geração de texto com MedGemma tendo como entrada texto e imagens.\"\"\"\n",
        "    if \"27b\" in self.model_version.lower():\n",
        "      raise ValueError(\"[ERRO] Essa função é inválida para esse modelo. O modelo MedGemma-27B é somente texto e não aceita imagens.\")\n",
        "\n",
        "    # Garante que image_inputs é uma lista\n",
        "    if not isinstance(image_inputs, list):\n",
        "        image_inputs = [image_inputs]\n",
        "\n",
        "    # Carrega todas as imagens\n",
        "    images = []\n",
        "    for img_input in image_inputs:\n",
        "        img = self.load_image(img_input)\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "        else:\n",
        "            print(f\"[ERRO] Falha ao carregar: {img_input}\")\n",
        "\n",
        "    if not images:\n",
        "        raise ValueError(\"[ERRO] Nenhuma imagem válida foi carregada. Abortando requisição.\")\n",
        "\n",
        "    message = [\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': [\n",
        "                {\n",
        "                    'type': 'text',\n",
        "                    'text': text\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    if sys_prompt is not None:\n",
        "      message.insert(0, {\n",
        "          'role': 'system',\n",
        "          'content': [\n",
        "              {\n",
        "                  'type': 'text',\n",
        "                  'text': sys_prompt\n",
        "              }\n",
        "          ]\n",
        "      })\n",
        "\n",
        "    # Adiciona imagens ao prompt\n",
        "    pos = 0 if sys_prompt is None else 1\n",
        "    for img in images:\n",
        "      message[pos][\"content\"].append({\"type\": \"image\", \"image\": img})\n",
        "\n",
        "    inputs = self.processor.apply_chat_template(\n",
        "        message,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(self.model.device, dtype=torch.bfloat16)\n",
        "\n",
        "    # Geração\n",
        "    outputs = self.model.generate(**inputs, max_new_tokens=max_tokens)\n",
        "    response = self.processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    if show_result:\n",
        "        print(\"\\n[RESPOSTA DO MODELO]:\\n\", response)\n",
        "\n",
        "    return response\n",
        "\n",
        "  def load_image(self, image_input):\n",
        "    \"\"\"\n",
        "      Carrega uma ou mais imagens (a partir de caminho, bytes ou BytesIO)\n",
        "      e converte todas para RGB (formato necessário para o MedGemma).\n",
        "      Retorna uma única imagem (PIL.Image) ou uma lista de imagens, conforme o input.\n",
        "    \"\"\"\n",
        "    def _load_single_image(single_input):\n",
        "        \"\"\"Carrega uma única imagem e converte para RGB.\"\"\"\n",
        "        if isinstance(single_input, (str, os.PathLike)):\n",
        "            if not os.path.exists(single_input):\n",
        "                raise FileNotFoundError(f\"O arquivo de imagem '{single_input}' não foi encontrado.\")\n",
        "            return Image.open(single_input).convert('RGB')\n",
        "\n",
        "        elif isinstance(single_input, (bytes, bytearray, io.BytesIO)):\n",
        "            if isinstance(single_input, (bytes, bytearray)):\n",
        "                single_input = io.BytesIO(single_input)\n",
        "            return Image.open(single_input).convert('RGB')\n",
        "\n",
        "        else:\n",
        "            raise TypeError(\"Cada elemento deve ser um caminho (str) ou um objeto de arquivo em memória.\")\n",
        "\n",
        "    try:\n",
        "        # Caso o input seja uma lista ou tupla de imagens\n",
        "        if isinstance(image_input, (list, tuple)):\n",
        "            images = []\n",
        "            for idx, img in enumerate(image_input):\n",
        "                try:\n",
        "                    loaded = _load_single_image(img)\n",
        "                    images.append(loaded)\n",
        "                except Exception as e:\n",
        "                    print(f\"[ERRO] Falha ao carregar imagem {idx + 1}: {e}\")\n",
        "            return images  # lista de imagens\n",
        "\n",
        "        # Caso seja apenas uma imagem\n",
        "        else:\n",
        "            return _load_single_image(image_input)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERRO] Falha ao carregar imagem(s): {e}\")\n",
        "        return None\n",
        "\n",
        "  def show_results(self, text, answer, image=None) -> None:\n",
        "    display(Markdown(f\"---\\n\\n**[ Requisição ]**\\n\\n{text}\"))\n",
        "    if image:\n",
        "      if isinstance(image, list):\n",
        "        for idx, img in enumerate(image, start=1):\n",
        "          display(Markdown(f\"**Imagem {idx}:**\"))\n",
        "          display(img)\n",
        "      else:\n",
        "          display(image)\n",
        "    display(Markdown(f\"---\\n\\n**[ MedGemma ]**\\n\\n{answer}\\n\\n---\"))\n",
        "\n",
        "  def select_device(self, device: str = None) -> str:\n",
        "    \"\"\"\n",
        "      Verifica se o device está disponível e retorna a string correspondente.\n",
        "      Caso device seja None, tenta usar GPU (CUDA) se disponível, senão CPU.\n",
        "      Exibe avisos recomendando o uso de GPU.\n",
        "    \"\"\"\n",
        "    # Caso o usuário tenha especificado manualmente\n",
        "    if device is not None:\n",
        "      if device.startswith(\"cuda\") and not torch.cuda.is_available():\n",
        "        print(\"[AVISO] CUDA foi especificado, mas nenhuma GPU está disponível. Alternando para CPU.\")\n",
        "        return \"cpu\"\n",
        "      elif device == \"cpu\":\n",
        "        print(\"[AVISO] O modelo será executado na CPU. Isso pode ser significativamente mais lento.\")\n",
        "        print(\"[SUGESTÃO] Considere usar uma GPU (CUDA) para acelerar a inferência.\")\n",
        "        return \"cpu\"\n",
        "      else:\n",
        "        return device\n",
        "\n",
        "    # Seleção automática\n",
        "    if torch.cuda.is_available():\n",
        "      print(\"[INFO] GPU CUDA detectada. Utilizando GPU para melhor desempenho.\")\n",
        "      return \"cuda\"\n",
        "    else:\n",
        "      print(\"[AVISO] Nenhuma GPU detectada. O modelo será executado na CPU.\")\n",
        "      print(\"[SUGESTÃO] Considere usar uma GPU (CUDA) para acelerar a execução.\")\n",
        "      return \"cpu\"\n",
        "\n",
        "  def config_model_params(self, device:str, use_quantization:bool) -> dict:\n",
        "    # Definir os parâmetros do modelo baseado no ambiente do Colab\n",
        "\n",
        "    # Caso dos modelos de 27B\n",
        "    if '27b' in self.model_version:\n",
        "      if 'A100' in torch.cuda.get_device_name(0):\n",
        "        # Parâmetros utilizados pelo exemplo da própria Google\n",
        "        model_kwargs = dict(\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map='auto',\n",
        "            quantization_config=BitsAndBytesConfig(\n",
        "                load_in_4bit=True # diminuição dos pesos para 4bits\n",
        "            )\n",
        "        )\n",
        "      else:\n",
        "        raise ValueError(\n",
        "            \"Runtime has insufficient memory to run a 27B variant.\"\n",
        "            \"Please select an A100 GPU.\"\n",
        "        )\n",
        "    # Caso dos modelos de 4B\n",
        "    elif '4b' in self.model_version:\n",
        "      if device == 'cpu': # apenas para depurar\n",
        "        # Nesse modo a RAM fica muito próxima do seu limite\n",
        "        model_kwargs = dict(\n",
        "            device_map='cpu',\n",
        "            # torch_dtype='float32',  # precisão máxima (está estourando o limite da RAM, 12.7GB)\n",
        "            torch_dtype=torch.float16, # Deve utilizar essa precisão para não estourar a RAM\n",
        "            low_cpu_mem_usage=True  # otimiza carregamento\n",
        "        )\n",
        "      elif device == 'cuda' and not use_quantization:\n",
        "        # uso estimado de 10~12GB de VRAM\n",
        "        # Ao utilizar esse modo a VRAM atingiu ~11GB de 15GB\n",
        "        model_kwargs = dict(\n",
        "            device_map='auto',\n",
        "            torch_dtype=torch.bfloat16  # mais leve que float32\n",
        "        )\n",
        "      elif device == 'cuda' and use_quantization:\n",
        "        # uso estimado de 4~6GB de VRAM\n",
        "        # Utiliza ~4GB da VRAM\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type='nf4'\n",
        "        )\n",
        "        model_kwargs = dict(\n",
        "            device_map='auto',\n",
        "            quantization_config=quant_config\n",
        "        )\n",
        "      else:\n",
        "        print(f'No valid option')\n",
        "        return None\n",
        "\n",
        "    print(f'Parâmetros do Modelo:\\n{model_kwargs=}')\n",
        "    return model_kwargs"
      ],
      "metadata": {
        "id": "Fryica4EDa-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Version2"
      ],
      "metadata": {
        "id": "k1Fv4OKFXHxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MedGemma():\n",
        "\n",
        "  models = {\n",
        "      '4b-pt' : 'google/medgemma-4b-pt',\n",
        "      '4b-it' : 'google/medgemma-4b-it',\n",
        "      '27b-mult' : 'google/medgemma-27b-it', # versão Multimodal\n",
        "      '27b-text' : 'google/medgemma-27b-text-it' # versão Texto\n",
        "  }\n",
        "\n",
        "  '''\n",
        "\n",
        "    * função específica que processa os dados (tanto textos quanto imagens)\n",
        "\n",
        "    * função para aceitar entrada dict e retornar dict com resposta\n",
        "\n",
        "  '''\n",
        "\n",
        "  def __init__(self, model_version:str='4b-it', device:str=None,\n",
        "             quantization:bool=False, model=None, processor=None):\n",
        "\n",
        "    self.device = self.select_device(device)\n",
        "    self.model_version = self.models[model_version]\n",
        "    self.model_params = self.config_model_params(self.device, quantization)\n",
        "\n",
        "    # Modelo\n",
        "    if model is not None:\n",
        "        self.model = model\n",
        "        print(f\"[INFO] Using preloaded model: {getattr(model, 'name_or_path', 'unknown')}\")\n",
        "    else:\n",
        "        print(f\"[INFO] Initializing model: {self.model_version}\")\n",
        "        if '4b' in self.model_version.lower():\n",
        "            self.model = AutoModelForImageTextToText.from_pretrained(\n",
        "                self.model_version,\n",
        "                **self.model_params\n",
        "                )\n",
        "        elif '27b' in self.model_version.lower():\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_version,\n",
        "                **self.model_params\n",
        "                )\n",
        "        else:\n",
        "            raise ValueError(f\"[ERROR] Invalid model version: {self.model_version}\")\n",
        "\n",
        "    # Processor\n",
        "    if processor is not None:\n",
        "        self.processor = processor\n",
        "        print(f\"[INFO] Using preloaded processor: {getattr(processor, 'name_or_path', 'unknown')}\")\n",
        "    else:\n",
        "        print(f\"[INFO] Initializing processor: {self.model_version}\")\n",
        "        if '4b' in self.model_version.lower():\n",
        "            self.processor = AutoProcessor.from_pretrained(\n",
        "                self.model_version,\n",
        "                use_fast=False\n",
        "                )\n",
        "        elif '27b' in self.model_version.lower():\n",
        "            self.processor = AutoTokenizer.from_pretrained(\n",
        "                self.model_version\n",
        "                )\n",
        "        else:\n",
        "            raise ValueError(f\"[ERROR] Invalid processor version: {self.model_version}\")\n",
        "\n",
        "  def return_model(self):\n",
        "    return self.model\n",
        "\n",
        "  def return_processor(self):\n",
        "    return self.processor\n",
        "\n",
        "  def processor_data(self, message, dtype=None):\n",
        "    inputs = self.processor.apply_chat_template(\n",
        "          message,\n",
        "          add_generation_prompt=True,\n",
        "          tokenize=True,\n",
        "          return_dict=True,\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "\n",
        "    if dtype:\n",
        "      return inputs.to(self.model.device, dtype=dtype)\n",
        "\n",
        "    return inputs.to(self.model.device)\n",
        "\n",
        "  def send_only_text(self, text:str, sys_prompt:str=None, max_tokens:int=300, show_result:bool=True) -> str:\n",
        "    ''' Geração de texto com MedGemma tendo como entrada apenas texto'''\n",
        "    # estrutura de mensagem\n",
        "    message = [\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': [\n",
        "                {\n",
        "                    'type': 'text',\n",
        "                    'text': text\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    # inserir prompt de sistema (opcional)\n",
        "    if sys_prompt is not None:\n",
        "      message.insert(0, {\n",
        "          'role': 'system',\n",
        "          'content': [\n",
        "              {\n",
        "                  'type': 'text',\n",
        "                  'text': sys_prompt\n",
        "              }\n",
        "          ]\n",
        "      })\n",
        "\n",
        "    print(f'[INFO] Message send to model: {message}')\n",
        "    # Preparação da entrada para o modelo\n",
        "    inputs = self.processor_data(message)\n",
        "\n",
        "    # tamanho da entrada (para removermos dos tokens de saída)\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "    # geração\n",
        "    with torch.inference_mode():\n",
        "      generation = self.model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False)\n",
        "      generation = generation[0][input_len:]\n",
        "\n",
        "    # decodificação\n",
        "    answer = self.processor.decode(generation, skip_special_tokens=True)\n",
        "    # visualização\n",
        "    if show_result:\n",
        "      self.show_results(text, answer)\n",
        "\n",
        "    return answer\n",
        "\n",
        "  def send_image_and_text(self, text, image_inputs, sys_prompt=None, max_tokens=256, show_result=True):\n",
        "    \"\"\"Geração de texto com MedGemma tendo como entrada texto e imagens.\"\"\"\n",
        "    if \"27b\" in self.model_version.lower():\n",
        "      raise ValueError(\"[ERRO] Essa função é inválida para esse modelo. O modelo MedGemma-27B é somente texto e não aceita imagens.\")\n",
        "\n",
        "    # Garante que image_inputs é uma lista\n",
        "    if not isinstance(image_inputs, list):\n",
        "        image_inputs = [image_inputs]\n",
        "\n",
        "    # Carrega todas as imagens\n",
        "    images = []\n",
        "    for img_input in image_inputs:\n",
        "        img = self.load_image(img_input)\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "        else:\n",
        "            print(f\"[ERRO] Falha ao carregar: {img_input}\")\n",
        "\n",
        "    if not images:\n",
        "        raise ValueError(\"[ERRO] Nenhuma imagem válida foi carregada. Abortando requisição.\")\n",
        "\n",
        "    message = [\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': [\n",
        "                {\n",
        "                    'type': 'text',\n",
        "                    'text': text\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "    if sys_prompt is not None:\n",
        "      message.insert(0, {\n",
        "          'role': 'system',\n",
        "          'content': [\n",
        "              {\n",
        "                  'type': 'text',\n",
        "                  'text': sys_prompt\n",
        "              }\n",
        "          ]\n",
        "      })\n",
        "\n",
        "    # Adiciona imagens ao prompt\n",
        "    pos = 0 if sys_prompt is None else 1\n",
        "    for img in images:\n",
        "      message[pos][\"content\"].append({\"type\": \"image\", \"image\": img})\n",
        "\n",
        "    inputs = self.processor_data(message, dtype=torch.bfloat16)\n",
        "\n",
        "    # Geração\n",
        "    outputs = self.model.generate(**inputs, max_new_tokens=max_tokens)\n",
        "    response = self.processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    if show_result:\n",
        "        print(\"\\n[RESPOSTA DO MODELO]:\\n\", response)\n",
        "\n",
        "    return response\n",
        "\n",
        "  def load_image(self, image_input):\n",
        "    \"\"\"\n",
        "      Carrega uma ou mais imagens (a partir de caminho, bytes ou BytesIO)\n",
        "      e converte todas para RGB (formato necessário para o MedGemma).\n",
        "      Retorna uma única imagem (PIL.Image) ou uma lista de imagens, conforme o input.\n",
        "    \"\"\"\n",
        "    def _load_single_image(single_input):\n",
        "        \"\"\"Carrega uma única imagem e converte para RGB.\"\"\"\n",
        "        if isinstance(single_input, (str, os.PathLike)):\n",
        "            if not os.path.exists(single_input):\n",
        "                raise FileNotFoundError(f\"O arquivo de imagem '{single_input}' não foi encontrado.\")\n",
        "            return Image.open(single_input).convert('RGB')\n",
        "\n",
        "        elif isinstance(single_input, (bytes, bytearray, io.BytesIO)):\n",
        "            if isinstance(single_input, (bytes, bytearray)):\n",
        "                single_input = io.BytesIO(single_input)\n",
        "            return Image.open(single_input).convert('RGB')\n",
        "\n",
        "        else:\n",
        "            raise TypeError(\"Cada elemento deve ser um caminho (str) ou um objeto de arquivo em memória.\")\n",
        "\n",
        "    try:\n",
        "        # Caso o input seja uma lista ou tupla de imagens\n",
        "        if isinstance(image_input, (list, tuple)):\n",
        "            images = []\n",
        "            for idx, img in enumerate(image_input):\n",
        "                try:\n",
        "                    loaded = _load_single_image(img)\n",
        "                    images.append(loaded)\n",
        "                except Exception as e:\n",
        "                    print(f\"[ERRO] Falha ao carregar imagem {idx + 1}: {e}\")\n",
        "            return images  # lista de imagens\n",
        "\n",
        "        # Caso seja apenas uma imagem\n",
        "        else:\n",
        "            return _load_single_image(image_input)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERRO] Falha ao carregar imagem(s): {e}\")\n",
        "        return None\n",
        "\n",
        "  def show_results(self, text, answer, image=None) -> None:\n",
        "    display(Markdown(f\"---\\n\\n**[ Requisição ]**\\n\\n{text}\"))\n",
        "    if image:\n",
        "      if isinstance(image, list):\n",
        "        for idx, img in enumerate(image, start=1):\n",
        "          display(Markdown(f\"**Imagem {idx}:**\"))\n",
        "          display(img)\n",
        "      else:\n",
        "          display(image)\n",
        "    display(Markdown(f\"---\\n\\n**[ MedGemma ]**\\n\\n{answer}\\n\\n---\"))\n",
        "\n",
        "  def select_device(self, device: str = None) -> str:\n",
        "    \"\"\"\n",
        "      Verifica se o device está disponível e retorna a string correspondente.\n",
        "      Caso device seja None, tenta usar GPU (CUDA) se disponível, senão CPU.\n",
        "      Exibe avisos recomendando o uso de GPU.\n",
        "    \"\"\"\n",
        "    # Caso o usuário tenha especificado manualmente\n",
        "    if device is not None:\n",
        "      if device.startswith(\"cuda\") and not torch.cuda.is_available():\n",
        "        print(\"[AVISO] CUDA foi especificado, mas nenhuma GPU está disponível. Alternando para CPU.\")\n",
        "        return \"cpu\"\n",
        "      elif device == \"cpu\":\n",
        "        print(\"[AVISO] O modelo será executado na CPU. Isso pode ser significativamente mais lento.\")\n",
        "        print(\"[SUGESTÃO] Considere usar uma GPU (CUDA) para acelerar a inferência.\")\n",
        "        return \"cpu\"\n",
        "      else:\n",
        "        return device\n",
        "\n",
        "    # Seleção automática\n",
        "    if torch.cuda.is_available():\n",
        "      print(\"[INFO] GPU CUDA detectada. Utilizando GPU para melhor desempenho.\")\n",
        "      return \"cuda\"\n",
        "    else:\n",
        "      print(\"[AVISO] Nenhuma GPU detectada. O modelo será executado na CPU.\")\n",
        "      print(\"[SUGESTÃO] Considere usar uma GPU (CUDA) para acelerar a execução.\")\n",
        "      return \"cpu\"\n",
        "\n",
        "  def config_model_params(self, device:str, use_quantization:bool) -> dict:\n",
        "    # Definir os parâmetros do modelo baseado no ambiente do Colab\n",
        "\n",
        "    # Caso dos modelos de 27B\n",
        "    if '27b' in self.model_version:\n",
        "      if 'A100' in torch.cuda.get_device_name(0):\n",
        "        # Parâmetros utilizados pelo exemplo da própria Google\n",
        "        model_kwargs = dict(\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map='auto',\n",
        "            quantization_config=BitsAndBytesConfig(\n",
        "                load_in_4bit=True # diminuição de alguns pesos para 4bits\n",
        "            )\n",
        "        )\n",
        "      else:\n",
        "        raise ValueError(\n",
        "            \"Runtime has insufficient memory to run a 27B variant.\"\n",
        "            \"Please select an A100 GPU.\"\n",
        "        )\n",
        "    # Caso dos modelos de 4B\n",
        "    elif '4b' in self.model_version:\n",
        "      if device == 'cpu': # apenas para depurar\n",
        "        # Nesse modo a RAM fica muito próxima do seu limite\n",
        "        model_kwargs = dict(\n",
        "            device_map='cpu',\n",
        "            # torch_dtype='float32',  # precisão máxima (está estourando o limite da RAM, 12.7GB)\n",
        "            torch_dtype=torch.float16, # Deve utilizar essa precisão para não estourar a RAM\n",
        "            low_cpu_mem_usage=True  # otimiza carregamento\n",
        "        )\n",
        "      elif device == 'cuda' and not use_quantization:\n",
        "        # uso estimado de 10~12GB de VRAM\n",
        "        # Ao utilizar esse modo a VRAM atingiu ~11GB de 15GB\n",
        "        model_kwargs = dict(\n",
        "            device_map='auto',\n",
        "            torch_dtype=torch.bfloat16  # mais leve que float32\n",
        "        )\n",
        "      elif device == 'cuda' and use_quantization:\n",
        "        # uso estimado de 4~6GB de VRAM\n",
        "        # Utiliza ~4GB da VRAM\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type='nf4'\n",
        "        )\n",
        "        model_kwargs = dict(\n",
        "            device_map='auto',\n",
        "            quantization_config=quant_config\n",
        "        )\n",
        "      else:\n",
        "        print(f'No valid option')\n",
        "        return None\n",
        "\n",
        "    print(f'Parâmetros do Modelo:\\n{model_kwargs=}')\n",
        "    return model_kwargs"
      ],
      "metadata": {
        "id": "DpDO-rNAXJJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## HuggingFace login"
      ],
      "metadata": {
        "id": "O2i4c4G12gUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "login()"
      ],
      "metadata": {
        "id": "Pqe2SIa8rxxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Experimentos"
      ],
      "metadata": {
        "id": "JyJDIVocLMi0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MedGemma4B"
      ],
      "metadata": {
        "id": "RLju-Upy1lQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "medgemma4b = UsingMedGemma()"
      ],
      "metadata": {
        "id": "uhm7lEht2GhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificando funcionamento"
      ],
      "metadata": {
        "id": "Ko-Mh-HUrdiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = []\n",
        "for root, _, files in os.walk('/content/'):\n",
        "  for file in files:\n",
        "    if file.lower().endswith('.jpg'):\n",
        "      image_path.append(os.path.join(root, file))\n",
        "print(image_path)"
      ],
      "metadata": {
        "id": "bP8u0l5HtbWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Português"
      ],
      "metadata": {
        "id": "NJ6-tJMGxrj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sys_prompt = 'Você é um especialista em radiografia'"
      ],
      "metadata": {
        "id": "qrIpfSMPzowm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_prompt = 'Descreva o raio-x abaixo. Por favor, responda em português (PT-BR)'"
      ],
      "metadata": {
        "id": "_R0-09rQrdO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "answer = medgemma4b.send_image_and_text(basic_prompt, image_path, sys_prompt)"
      ],
      "metadata": {
        "id": "6-OljWILt3_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "English"
      ],
      "metadata": {
        "id": "rZpb_w27yZYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sys_prompt_en = 'You are an specialist in radiograph'"
      ],
      "metadata": {
        "id": "YX4sZzEozyRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_prompt_en = 'Describe this X-ray'"
      ],
      "metadata": {
        "id": "jz7VPHj-yd0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "answer_en = answer = medgemma4b.send_image_and_text(basic_prompt_en, image_path, sys_prompt_en)"
      ],
      "metadata": {
        "id": "R0cX0GdWz3tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Tradução de documentos"
      ],
      "metadata": {
        "id": "6C5WsEoTuDn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_prompt = \"\"\"\n",
        "Traduza para o português o laudo abaixo:\n",
        "\n",
        "                                 FINAL REPORT\n",
        " EXAMINATION:  CHEST (PA AND LAT)\n",
        "\n",
        " INDICATION:  ___F with new onset ascites  // eval for infection\n",
        "\n",
        " TECHNIQUE:  Chest PA and lateral\n",
        "\n",
        " COMPARISON:  None.\n",
        "\n",
        " FINDINGS:\n",
        "\n",
        " There is no focal consolidation, pleural effusion or pneumothorax.  Bilateral\n",
        " nodular opacities that most likely represent nipple shadows. The\n",
        " cardiomediastinal silhouette is normal.  Clips project over the left lung,\n",
        " potentially within the breast. The imaged upper abdomen is unremarkable.\n",
        " Chronic deformity of the posterior left sixth and seventh ribs are noted.\n",
        "\n",
        " IMPRESSION:\n",
        "\n",
        " No acute cardiopulmonary process.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "g71WEUVuuFXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "answer_text = medgemma4b.send_only_text(text_prompt)"
      ],
      "metadata": {
        "id": "JJD4rvtUw6Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MedGemma27B (text-only)"
      ],
      "metadata": {
        "id": "ZZVwWQ1Y1yfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "medgemma27b = MedGemma('27b-text')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XJ9I3GURrXCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copiando model e processor"
      ],
      "metadata": {
        "id": "EDL4M9Baqf5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model27b = medgemma27b.return_model()"
      ],
      "metadata": {
        "id": "KEpkmVvjpm6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor27b = medgemma27b.return_processor()"
      ],
      "metadata": {
        "id": "-lKnrDRRpquo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_prompt = \"\"\"\n",
        "Traduza para o português o laudo abaixo:\n",
        "\n",
        "                                 FINAL REPORT\n",
        " EXAMINATION:  CHEST (PA AND LAT)\n",
        "\n",
        " INDICATION:  ___F with new onset ascites  // eval for infection\n",
        "\n",
        " TECHNIQUE:  Chest PA and lateral\n",
        "\n",
        " COMPARISON:  None.\n",
        "\n",
        " FINDINGS:\n",
        "\n",
        " There is no focal consolidation, pleural effusion or pneumothorax.  Bilateral\n",
        " nodular opacities that most likely represent nipple shadows. The\n",
        " cardiomediastinal silhouette is normal.  Clips project over the left lung,\n",
        " potentially within the breast. The imaged upper abdomen is unremarkable.\n",
        " Chronic deformity of the posterior left sixth and seventh ribs are noted.\n",
        "\n",
        " IMPRESSION:\n",
        "\n",
        " No acute cardiopulmonary process.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KKZlRzjz2QR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "answer_text = medgemma27b.send_only_text(text_prompt)"
      ],
      "metadata": {
        "id": "D_A_r7G-2SFe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys_prompt = 'Você é um tradutor de inglês para português brasileiro. Apenas traduza os textos que receber sem explicar.'"
      ],
      "metadata": {
        "id": "hq8V1jvOZKfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "answer_text = medgemma27b.send_only_text(text_prompt, sys_prompt)"
      ],
      "metadata": {
        "id": "2xMp5cauZJ3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_text"
      ],
      "metadata": {
        "id": "TB89M8J3q0PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Engineering"
      ],
      "metadata": {
        "id": "VEDSjqvdMc8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para estes experimentos irei utilizar o MedGemma4B, pois é menor e mais rápido."
      ],
      "metadata": {
        "id": "xVe66mdYMmKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "if 'medgemma4b' not in locals():\n",
        "  medgemma = UsingMedGemma()\n",
        "else:\n",
        "  medgemma = medgemma4b"
      ],
      "metadata": {
        "id": "8Bk3QdUWMlCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 1"
      ],
      "metadata": {
        "id": "V1YN3RfNN3Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt 1\n",
        "# Deixar claro que é um modelo tradudor de EN -> PT-BR e pedir para traduzir\n",
        "sys_prompt_1 = 'Você é um modelo especialista em traduções de inglês para português brasileiro.'\n",
        "text_prompt_1 = 'Traduza o texto abaixo para português brasileiro:\\n'\n",
        "# Texto a ser traduzido\n",
        "text_en = '''\n",
        "                                 FINAL REPORT\n",
        " EXAMINATION:  CHEST (PA AND LAT)\n",
        "\n",
        " INDICATION:  ___F with new onset ascites  // eval for infection\n",
        "\n",
        " TECHNIQUE:  Chest PA and lateral\n",
        "\n",
        " COMPARISON:  None.\n",
        "\n",
        " FINDINGS:\n",
        "\n",
        " There is no focal consolidation, pleural effusion or pneumothorax.  Bilateral\n",
        " nodular opacities that most likely represent nipple shadows. The\n",
        " cardiomediastinal silhouette is normal.  Clips project over the left lung,\n",
        " potentially within the breast. The imaged upper abdomen is unremarkable.\n",
        " Chronic deformity of the posterior left sixth and seventh ribs are noted.\n",
        "\n",
        " IMPRESSION:\n",
        "\n",
        " No acute cardiopulmonary process.\n",
        "'''\n",
        "text_prompt_1 = text_prompt_1 + text_en"
      ],
      "metadata": {
        "id": "IQQECHMHMv_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans_prompt1 = medgemma.send_only_text(text_prompt_1, sys_prompt_1, 500)"
      ],
      "metadata": {
        "id": "uNCibmjyNwnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 2"
      ],
      "metadata": {
        "id": "ALF3bKSwN4j5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt 2\n",
        "# Deixar claro que é um tradutor de EN -> PT-BR e também um especialista em laudos médicos.\n",
        "sys_prompt_2 = 'Você é um tradutor de inglês para português brasileiro. Você também é um especialista em laudos médicos.'\n",
        "text_prompt_2 = 'Traduza o texto abaixo para português brasileiro:\\n'\n",
        "# Texto a ser traduzido\n",
        "# text_en = ''\n",
        "text_prompt_2 = text_prompt_2 + text_en"
      ],
      "metadata": {
        "id": "BM62sE20N5hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans_prompt2 = medgemma.send_only_text(text_prompt_2, sys_prompt_2, 500)"
      ],
      "metadata": {
        "id": "ZcbEMQV-OdLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Traduções\n",
        "\n",
        "Arquivo selecionado do Google Drive e JSON salvo na mesma pasta"
      ],
      "metadata": {
        "id": "uOZ5F1c7PtzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acessar conteúdos do GDrive"
      ],
      "metadata": {
        "id": "aK4u-7OzRmb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cQDAXMHQRho9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caminho para arquivo json com os textos"
      ],
      "metadata": {
        "id": "nEO00_C81c4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# _files_path = input('Insert json path:\\n')\n",
        "_files_path = '/content/drive/MyDrive/2025_2S_IA368HH/Projeto Final/SubDataset - MIMIC-CXR/30_samples_only_text/medical_texts.json'"
      ],
      "metadata": {
        "id": "du3pF1Bt1hSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importar dados (JSON dos dados a serem traduzidos)"
      ],
      "metadata": {
        "id": "n_KIeiGaRofT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def import_json_from_drive(file_path: str=_files_path) -> dict:\n",
        "    \"\"\"\n",
        "    Importa um arquivo JSON do Google Drive (ou local) e retorna como dicionário.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Caminho completo do arquivo JSON.\n",
        "                         Exemplo: '/content/drive/MyDrive/pasta/arquivo.json'\n",
        "\n",
        "    Returns:\n",
        "        dict: Dados carregados do arquivo JSON.\n",
        "    \"\"\"\n",
        "    # Garante que o Google Drive está montado\n",
        "    drive_path = '/content/drive'\n",
        "    if not os.path.exists(drive_path):\n",
        "        drive.mount(drive_path)\n",
        "        print(\"Google Drive montado com sucesso.\")\n",
        "\n",
        "    # Verifica se o arquivo existe\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Arquivo não encontrado: {file_path}\")\n",
        "\n",
        "    # Lê o JSON\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        try:\n",
        "            data = json.load(f)\n",
        "            print(f\"Arquivo JSON carregado com sucesso de: {file_path}\")\n",
        "            print(f\"Total de registros: {len(data)}\")\n",
        "            return data\n",
        "        except json.JSONDecodeError as e:\n",
        "            raise ValueError(f\"Erro ao decodificar o JSON: {e}\")"
      ],
      "metadata": {
        "id": "7lPfuKVWRtu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "medical_texts = import_json_from_drive(_files_path)"
      ],
      "metadata": {
        "id": "SATzu8i_Aner"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "medical_texts['s50269882']"
      ],
      "metadata": {
        "id": "SwGs9GUxFwDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tradução dos dados (com armazenamentos frequentes)"
      ],
      "metadata": {
        "id": "DZhPT4CpSHdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translate_file_name = input('Enter the name of the json file that will contain the translated samples (Leave it as None if you want to use the default name):\\n')\n",
        "if len(translate_file_name) < 2:\n",
        "  translate_file_name = f'medical_texts_27b.json' if 'medgemma27b' in locals() else f'medical_texts_4b.json'\n",
        "if '.json' not in translate_file_name:\n",
        "  translate_file_name += '.json'\n",
        "print(translate_file_name)"
      ],
      "metadata": {
        "id": "HKqOy7-j4v25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_save_path = os.path.join(os.path.dirname(_files_path), translate_file_name)\n",
        "print(_save_path)"
      ],
      "metadata": {
        "id": "BLnPR90i394r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_function(model, dataset: dict, sys_prompt: str, user_prompt: str,\n",
        "                       save_path: str = _save_path,\n",
        "                       overwrite: bool = True,\n",
        "                       max_tokens: int = 500,\n",
        "                       save_every: int = 5):\n",
        "    \"\"\"\n",
        "    Traduz textos usando o modelo e salva periodicamente o progresso no Google Drive.\n",
        "\n",
        "    Args:\n",
        "        model: Instância da classe do MedGemma (ou outro modelo).\n",
        "        dataset: Dicionário com os textos em inglês.\n",
        "        sys_prompt: Prompt de sistema (por ex: \"Você é um tradutor médico...\").\n",
        "        user_prompt: Prompt usado para gerar a tradução.\n",
        "        save_path: Caminho completo para salvar o JSON no Google Drive.\n",
        "        overwrite: Se True, sobrescreve os elementos existentes.\n",
        "        max_tokens: Número máximo de tokens de saída.\n",
        "        save_every: Quantas traduções fazer antes de salvar o progresso.\n",
        "    \"\"\"\n",
        "\n",
        "    # Garante que o diretório de saída exista\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    total = len(dataset)\n",
        "    for i, id in enumerate(tqdm(dataset, total=total, ncols=100), start=1):\n",
        "        # Pula se já estiver traduzido (se possuir elemento na chave portuguese)\n",
        "        if 'portuguese' in dataset[id] and dataset[id]['portuguese'] and not overwrite:\n",
        "            continue\n",
        "\n",
        "        text_prompt = dataset[id]['english']\n",
        "\n",
        "        try:\n",
        "            # Tradução\n",
        "            translated_text = model.send_only_text(\n",
        "                text=f\"{user_prompt}\\n{text_prompt}\",\n",
        "                sys_prompt=sys_prompt,\n",
        "                max_tokens=max_tokens,\n",
        "                show_result=False\n",
        "            )\n",
        "            dataset[id]['portuguese'] = translated_text.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERRO] Falha ao traduzir {id}: {e}\")\n",
        "            dataset[id]['portuguese'] = None # deixo vazia para tentar de novo\n",
        "\n",
        "        # Salvamento periódico\n",
        "        if i % save_every == 0 or i == total:\n",
        "            try:\n",
        "                with open(save_path, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
        "                print(f\"[SALVO] Progresso salvo em: {save_path} ({i}/{total})\")\n",
        "            except Exception as e:\n",
        "                print(f\"[ERRO] Falha ao salvar JSON: {e}\")\n",
        "            # Espera um pouco para não sobrecarregar o Drive\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(f\"\\n Tradução concluída. Arquivo final salvo em: {save_path}\")\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "tsvEUMwVPvck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_with_translate = translate_function(medgemma4b,\n",
        "                                            medical_texts,\n",
        "                                            'Você é um tradutor de inglês para Português Brasileiro',\n",
        "                                            'Traduza o texto abaixo:'\n",
        "                                            )"
      ],
      "metadata": {
        "id": "gBo3Z9knGJ3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tradução de algumas amostras\n",
        "\n",
        "Varrendo as 30 amostras aleatórias com MedGemma 4B e 27B."
      ],
      "metadata": {
        "id": "CF-ffWPY5znZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtendo JSON"
      ],
      "metadata": {
        "id": "cQri6Zag-ubh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_files = [\n",
        "    file\n",
        "    for root, _, files in os.walk('/content/')\n",
        "    for file in files\n",
        "    if file.lower().endswith('.json')\n",
        "][0] # obter apenas o primeiro arquivo lido\n",
        "print(json_files)"
      ],
      "metadata": {
        "id": "pa4xO8T95181"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(json_files, 'r', encoding='utf-8') as file:\n",
        "  medical_data = json.load(file)"
      ],
      "metadata": {
        "id": "tv9gVdEa6Bqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tradução (aqui o modelo de medgemma utilizado depende de qual foi inicializado anteriormente)"
      ],
      "metadata": {
        "id": "b4pct_PK-wfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'medgemma27b' in locals():\n",
        "  medgemma = medgemma27b\n",
        "elif 'medgemma4b' in locals():\n",
        "  medgemma = medgemma4b\n",
        "else:\n",
        "  medgemma = UsingMedGemma() # caso nenhuma esteja inicializada, será iniciada a versão 4B"
      ],
      "metadata": {
        "id": "0wKF6XD53AQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "sys_prompt = 'Você é um tradutor de inglês para português brasileiro. Traduza os textos que receber.'\n",
        "for id in tqdm(medical_data, ncols=100):\n",
        "  text_prompt = medical_data[id]['english']\n",
        "  medical_data[id]['portuguese'] = medgemma.send_only_text(text_prompt, sys_prompt, 500, False)"
      ],
      "metadata": {
        "id": "avm4TcHh69UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "medical_data"
      ],
      "metadata": {
        "id": "2a9VFEQC9LF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salvando os textos no JSON original"
      ],
      "metadata": {
        "id": "SIz9kWDq-q30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('medical_texts.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(medical_data, f, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "ljkBcp-U9nae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Consumo de GPU"
      ],
      "metadata": {
        "id": "BKS6LXVeB1xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "rJ4b3zJbB4J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Liberar VRAM e reduzir uso de GPU"
      ],
      "metadata": {
        "id": "V6iX0sLeCFCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "CoMORSoZCDFX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}